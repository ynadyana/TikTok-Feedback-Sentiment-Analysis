{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNI2UYQyja5K8BCUWRKTAmX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["CLEAN DATA FOR QUANTITATIVE"],"metadata":{"id":"VajdJG-2hjHc"}},{"cell_type":"code","source":["!pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"CECzqJT2fXpI","executionInfo":{"status":"ok","timestamp":1751795119209,"user_tz":-480,"elapsed":8949,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"c9a99048-b083-49da-b218-5731c3ba6b54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.14.1\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import re\n","import pandas as pd\n","import emoji\n","\n","DATA_PATH   = Path(\"DataPost.csv\")\n","OUTPUT_PATH = Path(\"data_hijabistahub_clean.csv\")\n","\n","def load_apify_dump(path: Path) -> pd.DataFrame:\n","    if path.suffix.lower() == \".csv\":\n","        return pd.read_csv(path)\n","    if path.suffix.lower() in (\".json\", \".jsonl\"):\n","        return pd.read_json(path, orient=\"records\", lines=True)\n","    raise ValueError(f\"Unsupported file type: {path.suffix}\")\n","\n","df = load_apify_dump(DATA_PATH)\n","\n","#Drop low-utility / internal columns\n","drop_cols = [\"input\",\"effectStickers/0/ID\", \"effectStickers/0/name\",\n","       \"effectStickers/0/stickerStats/useCount\", \"effectStickers/1/ID\",\n","       \"effectStickers/1/name\", \"effectStickers/1/stickerStats/useCount\",\n","       \"effectStickers/2/ID\", \"effectStickers/2/name\",\n","       \"effectStickers/2/stickerStats/useCount\", \"effectStickers/3/ID\",\n","       \"effectStickers/3/name\", \"effectStickers/3/stickerStats/useCount\",\n","       \"effectStickers/4/ID\", \"effectStickers/4/name\",\n","       \"effectStickers/4/stickerStats/useCount\"]\n","df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n","\n","#flatten column names  (/ and \\ â†’ _)\n","df.columns = df.columns.str.replace(r\"[\\\\/]\", \"_\", regex=True)\n","\n","#numeric engagement metrics\n","numeric_cols = [\n","    \"collectCount\",\n","    \"commentCount\",\n","    \"diggCount\",\n","    \"playCount\",\n","    \"shareCount\",\n","]\n","for col in numeric_cols:\n","    if col in df.columns:\n","        df[col] = (\n","            df[col]\n","              .astype(str)\n","              .str.replace(\",\", \"\", regex=False)      # \"1,234\" â†’ \"1234\"\n","              .replace(r\"^\\s*$\", pd.NA, regex=True)   # blank â†’ NA\n","              .astype(float)\n","        )\n","\n","\n","#Boolean flags\n","bool_cols = [\"isAd\", \"isMuted\", \"isPinned\", \"isSlideshow\", \"isSponsored\"]\n","for col in bool_cols:\n","    if col in df.columns:\n","        df[col] = df[col].astype(\"boolean\")\n","\n","\n","#Datetime parsing\n","if \"createTimeISO\" in df.columns:\n","    df[\"post_datetime\"] = pd.to_datetime(df[\"createTimeISO\"], errors=\"coerce\")\n","df.drop(columns=[c for c in (\"createTime\", \"createTimeISO\") if c in df.columns],\n","        inplace=True)\n","\n","\n","#Collapse hashtag columns into one string\n","hashtag_cols = [c for c in df.columns if re.fullmatch(r\"hashtags_\\d+_name\", c)]\n","if hashtag_cols:\n","    df[\"hashtags\"] = (\n","        df[hashtag_cols]\n","          .apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n","    )\n","    df.drop(columns=hashtag_cols, inplace=True)\n","\n","\n","#Collapse mention columns into one string\n","mention_cols = [c for c in df.columns if re.fullmatch(r\"mentions_\\d+\", c)]\n","if mention_cols:\n","    df[\"mentions\"] = (\n","        df[mention_cols]\n","          .apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n","    )\n","    df.drop(columns=mention_cols, inplace=True)\n","\n","def clean_caption_all(text):\n","    if pd.isna(text):\n","        return \"\"\n","\n","    text = text.lower()                             # lowercase\n","    text = re.sub(r\"\\.{2,}\", \".\", text)             # collapse dots\n","    text = re.sub(r\"\\s+\", \" \", text).strip()        # normalize spaces\n","    text = emoji.demojize(text)                     # ðŸ˜ â†’ :heart_eyes:\n","    text = re.sub(r\"http\\S+\", \"\", text)             # remove URLs\n","    text = re.sub(r\"@\\w+\", \"\", text)                # remove @mentions\n","    text = re.sub(r\"[^\\w\\s:]\", \"\", text)            # keep colon for emoji text\n","    text = re.sub(r\"\\s+\", \" \", text).strip()        # remove extra whitespace\n","    return text\n","\n","df['cleaned_text'] = df['text'].apply(clean_caption_all)\n","\n","#Save clean CSV\n","df.to_csv(OUTPUT_PATH, index=False)\n","print(f\"âœ…  Clean file written to {OUTPUT_PATH}  \"\n","      f\"({df.shape[0]:,d} rows Ã— {df.shape[1]} columns)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuBlmSdVWGef","executionInfo":{"status":"ok","timestamp":1751795168645,"user_tz":-480,"elapsed":674,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"8a3c930f-4a49-4d8f-bc05-9787dd38b2e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ…  Clean file written to data_hijabistahub_clean.csv  (447 rows Ã— 27 columns)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"DataComment.csv\")\n","print(df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTzQDENgXFRa","executionInfo":{"status":"ok","timestamp":1751701198936,"user_tz":-480,"elapsed":37,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"453b05cb-e64d-4b44-cb0d-4ddbdaae66d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['avatarThumbnail', 'cid', 'createTime', 'createTimeISO',\n","       'detailedMentions/0/nickName', 'detailedMentions/0/profileUrl',\n","       'detailedMentions/0/secUid', 'detailedMentions/0/userId',\n","       'detailedMentions/1/nickName', 'detailedMentions/1/profileUrl',\n","       'detailedMentions/1/secUid', 'detailedMentions/1/userId',\n","       'detailedMentions/2/nickName', 'detailedMentions/2/profileUrl',\n","       'detailedMentions/2/secUid', 'detailedMentions/2/userId', 'diggCount',\n","       'input', 'likedByAuthor', 'mentions/0', 'mentions/1', 'mentions/2',\n","       'pinnedByAuthor', 'repliesToId', 'replyCommentTotal',\n","       'submittedVideoUrl', 'text', 'uid', 'uniqueId', 'videoWebUrl'],\n","      dtype='object')\n"]}]},{"cell_type":"markdown","source":["SPLIT DATASET"],"metadata":{"id":"Ka4xQ3_77yp_"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# ------------------------------------------------------------------\n","# 1. Load the cleaned comment data\n","#    (replace with your actual path / filename)\n","# ------------------------------------------------------------------\n","DATA_PATH = \"HijabistahubComment_labelled.csv\"\n","df = pd.read_csv(DATA_PATH)\n","\n","# ------------------------------------------------------------------\n","# 2. Split into train (80 %) and un-labelled test (20 %)\n","#    â€¢ If you ALREADY have a 'sentiment' column, drop it before splitting\n","#      and join it back to the train set only.\n","#    â€¢ random_state fixes the shuffle order so the split is reproducible.\n","# ------------------------------------------------------------------\n","train_df, test_df = train_test_split(\n","    df,\n","    test_size=0.20,\n","    random_state=42,\n","    shuffle=True\n",")\n","\n","# ------------------------------------------------------------------\n","# 3. Make sure the test set really has no sentiment label\n","#    (Here we just add an empty column as a placeholder)\n","# ------------------------------------------------------------------\n","test_df[\"sentiment\"] = pd.NA          # or test_df.drop(columns=['sentiment'], inplace=True)\n","\n","# ------------------------------------------------------------------\n","# 4. Save both files\n","# ------------------------------------------------------------------\n","train_df.to_csv(\"comments_train_to_label.csv\", index=False)\n","test_df.to_csv(\"comments_test_unlabelled.csv\", index=False)\n","\n","print(f\"âœ… Train set: {len(train_df):,} rows â†’ comments_train_to_label.csv\")\n","print(f\"âœ… Un-labelled test set: {len(test_df):,} rows â†’ comments_test_unlabelled.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-TCm1wIe7zRz","executionInfo":{"status":"ok","timestamp":1752171649034,"user_tz":-480,"elapsed":1764,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"e787e80c-ed18-4375-e6e3-f32e9e18a17a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Train set: 713 rows â†’ comments_train_to_label.csv\n","âœ… Un-labelled test set: 179 rows â†’ comments_test_unlabelled.csv\n"]}]},{"cell_type":"markdown","source":["CLEAN DATA FOR COMMENTS (QUANLITATIVE)"],"metadata":{"id":"uAwHounDCAbK"}},{"cell_type":"code","source":["!pip install malaya\n","!pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Onpq24KBNBmh","executionInfo":{"status":"ok","timestamp":1751729344335,"user_tz":-480,"elapsed":20116,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"b6ff4dbc-1cc9-47dd-8afe-65cce9760658"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting malaya\n","  Downloading malaya-5.1.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting dateparser (from malaya)\n","  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from malaya) (1.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from malaya) (2.32.3)\n","Collecting unidecode (from malaya)\n","  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from malaya) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from malaya) (1.15.3)\n","Collecting ftfy (from malaya)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from malaya) (3.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from malaya) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from malaya) (4.67.1)\n","Collecting malaya-boilerplate>=0.0.25 (from malaya)\n","  Downloading malaya_boilerplate-0.0.25-py3-none-any.whl.metadata (702 bytes)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from malaya) (2024.11.6)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from malaya) (4.53.0)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from malaya-boilerplate>=0.0.25->malaya) (0.33.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->malaya) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->malaya) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser->malaya) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->malaya) (2025.2)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->malaya) (5.3.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->malaya) (0.2.13)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->malaya) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->malaya) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->malaya) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->malaya) (2025.6.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->malaya) (3.18.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->malaya) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->malaya) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->malaya) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->malaya) (0.5.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->malaya-boilerplate>=0.0.25->malaya) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->malaya-boilerplate>=0.0.25->malaya) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->malaya-boilerplate>=0.0.25->malaya) (1.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser->malaya) (1.17.0)\n","Downloading malaya-5.1.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading malaya_boilerplate-0.0.25-py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode, ftfy, dateparser, malaya-boilerplate, malaya\n","Successfully installed dateparser-1.2.2 ftfy-6.3.1 malaya-5.1.1 malaya-boilerplate-0.0.25 unidecode-1.4.0\n","Collecting emoji\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.14.1\n"]}]},{"cell_type":"code","source":["import html, unicodedata\n","import re, emoji, pandas as pd, malaya\n","import pandas as pd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9mTsj8TNA_3","executionInfo":{"status":"ok","timestamp":1751729409222,"user_tz":-480,"elapsed":27937,"user":{"displayName":"nur alyyana sofeya","userId":"04205162450386901009"}},"outputId":"dbaecfcb-abd5-46e9-8c79-956e74b5e629"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n","  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n","/usr/local/lib/python3.11/dist-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n","  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"]}]},{"cell_type":"code","source":["# Load your dataset\n","df = pd.read_csv('DataComment.csv')\n","\n","# Load your comment-level dataset\n","df = pd.read_csv(\"DataComment.csv\")\n","\n","# Flatten column names (replace / with _)\n","df.columns = df.columns.str.replace(r\"[\\\\/]\", \"_\", regex=True)\n","\n","# Drop unnecessary columns\n","drop_cols = [c for c in df.columns if any(key in c for key in [\n","    \"avatarThumbnail\", \"cid\", \"uid\", \"secUid\", \"profileUrl\", \"repliesToId\", \"input\"\n","])]\n","df.drop(columns=drop_cols, inplace=True)\n","\n","# Collapse mentions\n","mention_cols = [c for c in df.columns if c.startswith(\"mentions_\")]\n","if mention_cols:\n","    df[\"mentions\"] = df[mention_cols].apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n","    df.drop(columns=mention_cols, inplace=True)\n","\n","# Collapse mentions\n","mention_cols = [c for c in df.columns if c.startswith(\"detailedMentions_\")]\n","if mention_cols:\n","    df[\"detailedMentions\"] = df[mention_cols].apply(lambda row: \" \".join(row.dropna().astype(str)), axis=1)\n","    df.drop(columns=mention_cols, inplace=True)\n","\n","#convert datetime\n","df[\"comment_datetime\"] = pd.to_datetime(df[\"createTimeISO\"], errors=\"coerce\")\n","df.drop(columns=[\"createTime\", \"createTimeISO\"], inplace=True)\n","\n","# Load normalizer\n","normalizer = malaya.normalize.normalizer()\n","\n","# Remove weird unicode\n","def remove_weird_unicode(text):\n","    text = text.replace('Â²', '2')\n","    text = text.replace('Â¹', '1')\n","    return text\n","\n","def unicode_html_normalize(text: str) -> str:\n","    if not isinstance(text, str):\n","        return ''\n","    text = html.unescape(text)\n","    text = unicodedata.normalize('NFKC', text)\n","    text = text.replace('\\u200b', '')\n","    return text\n","\n","URL_PATTERN    = re.compile(r'https?://\\S+|www\\.\\S+')\n","EMAIL_PATTERN  = re.compile(r'\\b\\S+@\\S+\\.\\S+\\b')\n","\n","def mask_urls_emails(text: str) -> str:\n","    if not isinstance(text, str):\n","        return ''\n","    text = URL_PATTERN.sub('[URL]', text)\n","    text = EMAIL_PATTERN.sub('[EMAIL]', text)\n","    return text\n","\n","#remove any mention\n","def remove_user_mentions(text):\n","    if not isinstance(text, str):\n","        return text\n","    # Remove any @username pattern (alphanumeric, underscores, dots)\n","    return re.sub(r'@\\w+', '', text)\n","\n","def fix_apostrophes(text):\n","    return text.replace(\"â€™\", \"'\")\n","\n","contractions = {\n","    \"it's\": \"it is\",\n","    \"isn't\": \"is not\",\n","    \"don't\": \"do not\",\n","    \"can't\": \"cannot\",\n","    \"i'm\": \"i am\",\n","    \"you're\": \"you are\",\n","    \"they're\": \"they are\",\n","    \"we're\": \"we are\",\n","    \"didn't\": \"did not\",\n","    \"YOURE\" : \"you are\"\n","}\n","\n","def expand_contractions(text):\n","    for contraction, full in contractions.items():\n","        text = text.replace(contraction, full)\n","    return text\n","\n","# replace Malay shortforms\n","shortform_mapping = {\n","    'yg': 'yang', 'tk': 'tak', 'x': 'tak', 'xde': 'tiada',\n","    'takde': 'tak ada', 'tak de': 'tak ada', 'nk': 'nak',\n","    'gi': 'pergi', 'dgn': 'dengan', 'blh': 'boleh',\n","    'tp': 'tapi', 'ko': 'kau', 'lg': 'lagi', 'org': 'orang',\n","    'depa': 'mereka', 'tu': 'itu', 'sbb': 'sebab', 'msti': 'mesti',\n","    'ni': 'ini', 'bang': 'abang'\n","}\n","\n","def replace_shortforms(text):\n","    if not isinstance(text, str):\n","        return text\n","    for short, full in shortform_mapping.items():\n","        pattern = r'\\b' + re.escape(short) + r'\\b'  # Match whole word\n","        text = re.sub(pattern, full, text)\n","    return text\n","\n","#convert emojis to text\n","def convert_emoji_to_text(text):\n","    return emoji.demojize(text)\n","\n","#fix multiple punctuations\n","def fix_multiple_punctuations(text):\n","    text = re.sub(r'\\s*\\.\\s*', '.', text)  # fix dots\n","    text = re.sub(r'\\s*!+\\s*', '!', text)   # fix exclamations\n","    text = re.sub(r'\\s*\\?+\\s*', '?', text)  # fix questions\n","    text = re.sub(r'\\.{2,}', '.', text)  # multiple dots â†’ one dot\n","    text = re.sub(r'!{2,}', '!', text)   # multiple ! â†’ one !\n","    text = re.sub(r'\\?{2,}', '?', text)  # multiple ? â†’ one ?\n","    return text\n","\n","#expands double word\n","def expand_double_words(text):\n","    if not isinstance(text, str):\n","        return text\n","    # Only expand if \"ii\" is a standalone word (with spaces or punctuation)\n","    text = re.sub(r'\\b(\\w+)\\s+i{2}\\b', r'\\1 \\1', text)  # handles \"sama ii\"\n","    return text\n","\n","#Apply full cleaning pipeline\n","def full_cleaning_pipeline(text):\n","    if not isinstance(text, str):\n","        return ''\n","    text = unicode_html_normalize(text)\n","    text = mask_urls_emails(text)\n","    text = remove_user_mentions(text)\n","    text = remove_weird_unicode(text)\n","    text = fix_apostrophes(text)\n","    text = expand_contractions(text.lower())\n","    text = replace_shortforms(text)\n","    text = convert_emoji_to_text(text)\n","    text = expand_double_words(text)\n","    # Collapse stray whitespace that may have been introduced\n","    text = re.sub(r'\\s{2,}', ' ', text).strip()\n","    return text\n","\n","# Apply to create 'cleaned_text' column\n","df['cleaned_text'] = df['text'].apply(full_cleaning_pipeline)\n","\n","# Normalize using Malaya\n","def normalize_text(text):\n","    if not isinstance(text, str):\n","        return ''\n","    try:\n","        normalized = normalizer.normalize(text)\n","        if isinstance(normalized, dict):\n","            return normalized['normalize']\n","        else:\n","            return normalized\n","    except:\n","        return text\n","\n","df['cleaned_text'] = df['cleaned_text'].apply(normalize_text)\n","\n","# Fix multiple punctuation and lowercase\n","df['cleaned_text'] = df['cleaned_text'].apply(fix_multiple_punctuations)\n","df['cleaned_text'] = df['cleaned_text'].str.lower()\n","\n","#custom mapping that are done by manually check one by one the cleaned data if there\n","#is any mislook shortforms or mispell words that malaya couldnt catch\n","custom_mapping = {'abis':'habis','skali':'sekali','aja':'saja','siutlaa':'siotla','datok':'dato',\n","                  'collaberation':'collaboration','kawin':'kahwin','kawen':'kahwin','kawhinn':'kahwin',\n","                  'fakehopa':'fake hope','western':'western','diorng':'diorang','lpsnie':'lepasni',\n","                  'sorg':'sorang','dorang':'diorang','mmg':'memang','sy':'saya','dlu':'dulu','lak':'pulak',\n","                  'ku':'aku','tak':'tidak','nmpk':'nampak','mls':'malas','ambk':'ambil','datu-datu':'dato dato',\n","                  'pebenda':'apa benda','kne':'kena','ak':'aku','trik':'tarik','bapak':'bapa','mals':'malas',\n","                  'bpk':'bapa','gak':'juga','mcm':'macam','maleh':'malas','die':'dia','blkng':'belakang',\n","                  'jnji':'janji','lsg':'langsung','ktwa':'ketawa','ptut':'patut','sye':'saya','giler':'gila',\n","                  'tuh':'tu','cantek':'cantik','okey':'okay','ensemm':'hensem','sgt':'sangat','bru':'baru',\n","                  'bals':'balas','cam':'macam','btl-btl':'betul-betul','stu':'satu','klu':'kalau','nnti':'nanti',\n","                  'tgok':'tengok','datuk':'dato','ikam':'ikan','zizang':'zizan','tggu':'tunggu','jer':'je',\n","                  'punchlina':'punchline','byk':'banyak','msa':'masa','dlu':'dulu','ne':'ni','ntah':'entah',\n","                  'hbis':'habis','knpela':'kenapala','drpd':'daripada','laki':'lelaki','dye':'dia','pulokk':'pula',\n","                  'suprisa':'suprise','get back':'getback','niy':'ni','dr':'dari','bnr':'benar','zan':'zizan',\n","                  'ambik':'ambil','cntik':'cantik','logat':'loghat','bgus':'bagus','lurva':'lurve','sesunguh':'sesungguhnya',\n","                  'adek':'adik','cutta':'cute','aq':'aku','tgk':'tengok','spa':'siapa','lpsni':'lepasni','mlysia':'malaysia',\n","                  'gile':'gila','dieorang':'diorang','pkai':'pakai','klo':'kalau','internasional':'international',\n","                  'gilers':'gila','klatei':'kelantan','dio':'dia','kelate':'kelantan','taglina':'tagline','comey':'comel',\n","                  'mampuihh':'mampus','mmpos':'mampus','teruseesss':'terus','trus':'terus','prmpuan':'perempuan','gilos':'gila',\n","                  'adekk':'adik','lupe':'lupa','dorg':'diorang','canteknya':'cantiknya','coleb':'collab','ade':'ada',\n","                  'sunaa':'cuna','jy':'juga','kunen':'kuning','gak':'tak','tknk':'taknak','dorang':'diorang','alip':'alif',\n","                  'besaqq':'besar','canti':'cantik','daviena':'davina','ko':'kau','ari':'hari','sgt':'sangat',\n","                  'hamble':'humble','sppu':'sepupu','comeyy':'comel','syantiknyaa':'cantiknya','micel':'michelle',\n","                  'viba':'vibe','mekap':'makeup','pikap line':'pickupline','klate':'kelantan',\n","                  'kiteorang':'kitaorang','hay':'hai','btl':'betul','jugak':'juga','vers':'versi','kawinn':'kahwin',\n","                  'gewa':'gewe','klo':'kalau'\n","                  }\n","\n","\n","# Function to replace only full words using regex\n","def replace_custom_shortforms(text, mapping):\n","    if not isinstance(text, str):\n","        return text\n","    for short, full in mapping.items():\n","        pattern = r'\\b' + re.escape(short) + r'\\b'\n","        text = re.sub(pattern, full, text)\n","    return text\n","\n","# Apply the function directly to cleaned_text\n","df['cleaned_text'] = df['cleaned_text'].apply(lambda x: replace_custom_shortforms(x, custom_mapping))\n","\n","\n","\n","df.to_csv(\"DataCommentTesting.csv\", index=False)\n"],"metadata":{"id":"OTTzcsJdMVLs"},"execution_count":null,"outputs":[]}]}